{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "探究代码如何实现loupe中的归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Memc_LOUPE(nn.Module):\n",
    "    def __init__(self, input_shape, slope, sample_slope, device, sparsity):\n",
    "        super(Memc_LOUPE, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.slope = slope \n",
    "        self.device = device\n",
    "        self.add_weight_real = nn.Parameter(- torch.log(1. / torch.rand(self.input_shape, dtype=torch.float32) - 1.) / self.slope, requires_grad=True)\n",
    "        self.add_weight_imag = nn.Parameter(- torch.log(1. / torch.rand(self.input_shape, dtype=torch.float32) - 1.) / self.slope, requires_grad=True)\n",
    "        self.sample_slope = sample_slope\n",
    "        self.sparsity = sparsity\n",
    "        self.conv = nn.Conv2d(4, 2, 1, 1, 0)\n",
    "\n",
    "    def calculate_Mask(self, kspace_mc, option):\n",
    "        print('sparsity:',self.sparsity)\n",
    "        logit_weights_real = 0 * kspace_mc[:,0, :, :] + self.add_weight_real\n",
    "        logit_weights_imag = 0 * kspace_mc[:,1, :, :] + self.add_weight_imag\n",
    "        prob_mask_tensor = torch.cat((logit_weights_real, logit_weights_imag), dim=1)\n",
    "        prob_mask_tensor = self.conv(prob_mask_tensor)\n",
    "        prob_mask_tensor = torch.sigmoid(self.slope * prob_mask_tensor)\n",
    "        #分析变量对应关系\n",
    "        \n",
    "        xbar = torch.mean(prob_mask_tensor)\n",
    "        r = self.sparsity / xbar\n",
    "        beta = (1 - self.sparsity) / (1 - xbar)\n",
    "        le = (torch.less_equal(r, 1)).to(dtype=torch.float32)\n",
    "        prob_mask_tensor = le * prob_mask_tensor * r + (1 - le) * (1 - (1 - prob_mask_tensor) * beta)\n",
    "        threshs = torch.rand(prob_mask_tensor.size(), dtype=torch.float32).to(device=self.device)\n",
    "        thresh_tensor = 0 * prob_mask_tensor + threshs\n",
    "\n",
    "        last_tensor_mask = torch.sigmoid(self.sample_slope * (prob_mask_tensor - thresh_tensor)) \n",
    "        return last_tensor_mask.to(device=self.device)\n",
    "\n",
    "    def forward(self,mask):\n",
    "        B,C,H,W=mask.shape\n",
    "        assert H==256 and W==256\n",
    "        dcmask = self.calculate_Mask(mask, option=True)#inital work\n",
    "        return  dcmask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "探究nn.Parameter生成的参数是否是同时优化，同一个梯度不同的初始值？还是不同的初始值？ 实际需要应该是按照每个点用不同的梯度进行更新，卷积？ 默认前面的已经学出来概率分布，看看归一化是怎么实现的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
